{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76f38b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/vp054179/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/vp054179/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/vp054179/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/vp054179/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import conll2000\n",
    "nltk.download('treebank')\n",
    "nltk.download('brown')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "34a5f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a305829",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c11e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load POS tagged corpora from nltk\n",
    "treebank_corpus = treebank.tagged_sents(tagset = 'universal')\n",
    "brown_corpus = brown.tagged_sents(tagset='universal')\n",
    "conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
    "\n",
    "tagged_sentences = treebank_corpus + brown_corpus + conll_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ccde8bb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DET'),\n",
       " ('Lorillard', 'NOUN'),\n",
       " ('spokewoman', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " (',', '.'),\n",
       " ('``', '.'),\n",
       " ('This', 'DET'),\n",
       " ('is', 'VERB'),\n",
       " ('an', 'DET'),\n",
       " ('old', 'ADJ'),\n",
       " ('story', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at the data\n",
    "tagged_sentences[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d0bf83",
   "metadata": {},
   "source": [
    "### Divide data in words (X) and tags (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ad3c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # store input sequence\n",
    "Y = [] # store output sequence\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    X_sentence = []\n",
    "    Y_sentence = []\n",
    "    for entity in sentence: \n",
    "        X_sentence.append(entity[0]) # entity[0] contains the word\n",
    "        Y_sentence.append(entity[1])  # entity[1] contains corresponding tag\n",
    "        \n",
    "    X.append(X_sentence)\n",
    "    Y.append(Y_sentence)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "749535be",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(set([word.lower() for sentence in X for word in sentence]))\n",
    "num_tags   = len(set([word.lower() for sentence in Y for word in sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9caa8622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tagged sentences: 72202\n",
      "Vocabulary size: 59448\n",
      "Total number of tags: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of tagged sentences: {}\".format(len(X)))\n",
    "print(\"Vocabulary size: {}\".format(num_words))\n",
    "print(\"Total number of tags: {}\".format(num_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc74faeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample X:  ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'] \n",
      "\n",
      "sample Y:  ['NOUN', 'NOUN', '.', 'NUM', 'NOUN', 'ADJ', '.', 'VERB', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', 'NUM', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's look at first data point\n",
    "# this is one data point that will be fed to the RNN\n",
    "print('sample X: ', X[0], '\\n')\n",
    "print('sample Y: ', Y[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fdc3c089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of first input sequence  : 18\n",
      "Length of first output sequence : 18\n"
     ]
    }
   ],
   "source": [
    "# In this many-to-many problem, the length of each input and output sequence must be the same.\n",
    "# Since each word is tagged, it's important to make sure that the length of input sequence equals the output sequence\n",
    "print(\"Length of first input sequence  : {}\".format(len(X[0])))\n",
    "print(\"Length of first output sequence : {}\".format(len(Y[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa90745",
   "metadata": {},
   "source": [
    "### Vectorise X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6067613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode X\n",
    "\n",
    "word_tokenizer = Tokenizer()                      # instantiate tokeniser\n",
    "word_tokenizer.fit_on_texts(X)                    # fit tokeniser on data\n",
    "X_encoded = word_tokenizer.texts_to_sequences(X)  # use the tokeniser to encode input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f39c2c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode Y\n",
    "\n",
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(Y)\n",
    "Y_encoded = tag_tokenizer.texts_to_sequences(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5979d73e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Raw data point ** \n",
      " ---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "X:  ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'] \n",
      "\n",
      "Y:  ['NOUN', 'NOUN', '.', 'NUM', 'NOUN', 'ADJ', '.', 'VERB', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', 'NUM', '.'] \n",
      "\n",
      "\n",
      "** Encoded data point ** \n",
      " ---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "X:  [6423, 24231, 2, 7652, 102, 170, 2, 47, 1898, 1, 269, 17, 7, 13230, 619, 1711, 2761, 3] \n",
      "\n",
      "Y:  [1, 1, 3, 11, 1, 6, 3, 2, 2, 5, 1, 4, 5, 6, 1, 1, 11, 3] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at first encoded data point\n",
    "\n",
    "print(\"** Raw data point **\", \"\\n\", \"-\"*100, \"\\n\")\n",
    "print('X: ', X[0], '\\n')\n",
    "print('Y: ', Y[0], '\\n')\n",
    "print()\n",
    "print(\"** Encoded data point **\", \"\\n\", \"-\"*100, \"\\n\")\n",
    "print('X: ', X_encoded[0], '\\n')\n",
    "print('Y: ', Y_encoded[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4cfc3958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sentences have disparate input-output lengths.\n"
     ]
    }
   ],
   "source": [
    "# make sure that each sequence of input and output is same length\n",
    "\n",
    "different_length = [1 if len(x) != len(y) else 0 for x, y in zip(X_encoded, Y_encoded)]\n",
    "print(\"{} sentences have disparate input-output lengths.\".format(sum(different_length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32dffc",
   "metadata": {},
   "source": [
    "### Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6eeb8032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of longest sentence: 271\n"
     ]
    }
   ],
   "source": [
    "# check length of longest sentence\n",
    "lengths = [len(seq) for seq in X_encoded]\n",
    "print(\"Length of longest sentence: {}\".format(max(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1c32ef0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ebce478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad each sequence to MAX_SEQ_LENGTH using KERAS' pad_sequences() function. \n",
    "# Sentences longer than MAX_SEQ_LENGTH are truncated.\n",
    "# Sentences shorter than MAX_SEQ_LENGTH are padded with zeroes.\n",
    "\n",
    "# Truncation and padding can either be 'pre' or 'post'. \n",
    "# For padding we are using 'pre' padding type, that is, add zeroes on the left side.\n",
    "# For truncation, we are using 'post', that is, truncate a sentence from right side.\n",
    "\n",
    "MAX_SEQ_LENGTH = 100  # sequences greater than 100 in length will be truncated\n",
    "\n",
    "X_padded = pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "Y_padded = pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8ac29c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0  6423 24231\n",
      "     2  7652   102   170     2    47  1898     1   269    17     7 13230\n",
      "   619  1711  2761     3] \n",
      "\n",
      "\n",
      "\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  1  1  3 11  1  6  3  2  2  5  1  4  5  6\n",
      "  1  1 11  3]\n"
     ]
    }
   ],
   "source": [
    "# print the first sequence\n",
    "print(X_padded[0], \"\\n\"*3)\n",
    "print(Y_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b09ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign padded sequences to X and Y\n",
    "X, Y = X_padded, Y_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2945c1c3",
   "metadata": {},
   "source": [
    "### Use word embeddings for input sequences (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b5f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # word2vec\n",
    "\n",
    "# path = '../input/wordembeddings/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "\n",
    "# # load word2vec using the following function present in the gensim library\n",
    "# word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "07e51353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assign word vectors from word2vec model\n",
    "\n",
    "EMBEDDING_SIZE  = 300  # each word in word2vec model is represented using a 300 dimensional vector\n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "# # create an empty embedding matix\n",
    "# embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "\n",
    "# # create a word to index dictionary mapping\n",
    "# word2id = word_tokenizer.word_index\n",
    "\n",
    "# # copy vectors from word2vec model to the words present in corpus\n",
    "# for word, index in word2id.items():\n",
    "#     try:\n",
    "#         embedding_weights[index, :] = word2vec[word]\n",
    "#     except KeyError:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1716f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check embedding dimension\n",
    "# print(\"Embeddings shape: {}\".format(embedding_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd19ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let's look at an embedding of a word\n",
    "# embedding_weights[word_tokenizer.word_index['joy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f2449a",
   "metadata": {},
   "source": [
    "### Use one-hot encoding for output sequences (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c2d4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Keras' to_categorical function to one-hot encode Y\n",
    "Y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1dd6218d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72202, 100, 13)\n"
     ]
    }
   ],
   "source": [
    "# print Y of the first output sequqnce\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cdb6be",
   "metadata": {},
   "source": [
    "### Split data in training, validation and tesing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e29ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split entire data into training and testing sets\n",
    "TEST_SIZE = 0.15\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ab0b8a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data into training and validation sets\n",
    "VALID_SIZE = 0.15\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=VALID_SIZE, random_state=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ec0ce30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Shape of input sequences: (52165, 100)\n",
      "Shape of output sequences: (52165, 100, 13)\n",
      "--------------------------------------------------\n",
      "VALIDATION DATA\n",
      "Shape of input sequences: (9206, 100)\n",
      "Shape of output sequences: (9206, 100, 13)\n",
      "--------------------------------------------------\n",
      "TESTING DATA\n",
      "Shape of input sequences: (10831, 100)\n",
      "Shape of output sequences: (10831, 100, 13)\n"
     ]
    }
   ],
   "source": [
    "# print number of samples in each set\n",
    "print(\"TRAINING DATA\")\n",
    "print('Shape of input sequences: {}'.format(X_train.shape))\n",
    "print('Shape of output sequences: {}'.format(Y_train.shape))\n",
    "print(\"-\"*50)\n",
    "print(\"VALIDATION DATA\")\n",
    "print('Shape of input sequences: {}'.format(X_validation.shape))\n",
    "print('Shape of output sequences: {}'.format(Y_validation.shape))\n",
    "print(\"-\"*50)\n",
    "print(\"TESTING DATA\")\n",
    "print('Shape of input sequences: {}'.format(X_test.shape))\n",
    "print('Shape of output sequences: {}'.format(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d4b8d",
   "metadata": {},
   "source": [
    "### 2. Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc37d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of tags\n",
    "NUM_CLASSES = Y.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f881d531",
   "metadata": {},
   "source": [
    "**First let's try running a vanilla RNN. For this RNN we won't use the pre-trained word embeddings. We'll use randomly inititalised embeddings. Moreover, we won't update the embeddings weights.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fe6b544e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 300)          17834700  \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 100, 64)           23360     \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 100, 13)          845       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,858,905\n",
      "Trainable params: 24,205\n",
      "Non-trainable params: 17,834,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create architecture\n",
    "\n",
    "rnn_model = Sequential()\n",
    "\n",
    "# create embedding layer - usually the first layer in text problems\n",
    "rnn_model.add(Embedding(input_dim     =  VOCABULARY_SIZE,         # vocabulary size - number of unique words in data\n",
    "                        output_dim    =  EMBEDDING_SIZE,          # length of vector with which each word is represented\n",
    "                        input_length  =  MAX_SEQ_LENGTH,          # length of input sequence\n",
    "                        trainable     =  False                    # False - don't update the embeddings\n",
    "))\n",
    "\n",
    "# add an RNN layer which contains 64 RNN cells\n",
    "rnn_model.add(SimpleRNN(64, \n",
    "              return_sequences=True  # True - return whole sequence; False - return single output of the end of the sequence\n",
    "))\n",
    "\n",
    "# add time distributed (output at each sequence) layer\n",
    "rnn_model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eee7ca8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59449"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCABULARY_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346cda9",
   "metadata": {},
   "source": [
    "### compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2606f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.compile(loss      =  'categorical_crossentropy',\n",
    "                  optimizer =  'adam',\n",
    "                  metrics   =  ['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c1497123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 300)          17834700  \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 100, 64)           23360     \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 100, 13)          845       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,858,905\n",
      "Trainable params: 24,205\n",
      "Non-trainable params: 17,834,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check summary of the model\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb670974",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ad2ccd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "408/408 [==============================] - 14s 33ms/step - loss: 0.5137 - acc: 0.8505 - val_loss: 0.3380 - val_acc: 0.8967\n",
      "Epoch 2/10\n",
      "408/408 [==============================] - 13s 32ms/step - loss: 0.2813 - acc: 0.9131 - val_loss: 0.2357 - val_acc: 0.9277\n",
      "Epoch 3/10\n",
      "408/408 [==============================] - 14s 33ms/step - loss: 0.2131 - acc: 0.9335 - val_loss: 0.1922 - val_acc: 0.9388\n",
      "Epoch 4/10\n",
      "408/408 [==============================] - 13s 32ms/step - loss: 0.1804 - acc: 0.9420 - val_loss: 0.1675 - val_acc: 0.9456\n",
      "Epoch 5/10\n",
      "408/408 [==============================] - 13s 32ms/step - loss: 0.1598 - acc: 0.9481 - val_loss: 0.1506 - val_acc: 0.9511\n",
      "Epoch 6/10\n",
      "408/408 [==============================] - 14s 33ms/step - loss: 0.1463 - acc: 0.9523 - val_loss: 0.1399 - val_acc: 0.9538\n",
      "Epoch 7/10\n",
      "408/408 [==============================] - 14s 33ms/step - loss: 0.1375 - acc: 0.9546 - val_loss: 0.1327 - val_acc: 0.9558\n",
      "Epoch 8/10\n",
      "408/408 [==============================] - 14s 33ms/step - loss: 0.1314 - acc: 0.9562 - val_loss: 0.1283 - val_acc: 0.9564\n",
      "Epoch 9/10\n",
      "408/408 [==============================] - 14s 35ms/step - loss: 0.1270 - acc: 0.9575 - val_loss: 0.1247 - val_acc: 0.9574\n",
      "Epoch 10/10\n",
      "408/408 [==============================] - 14s 34ms/step - loss: 0.1237 - acc: 0.9583 - val_loss: 0.1212 - val_acc: 0.9589\n"
     ]
    }
   ],
   "source": [
    "rnn_training = rnn_model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_validation, Y_validation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b155345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
